{
 "cells": [
  {
   "cell_type": "code",
   "id": "b4016c87a1ab8fdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:07:58.347932Z",
     "start_time": "2025-02-01T18:07:14.104122Z"
    }
   },
   "source": [
    "import torch\n",
    "import transformers\n",
    "import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Load LLaMA 3.3-70B-Instruct model from Hugging Face\n",
    "model_name = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "hf_token = \"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\"\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "\n",
    "# Enable 4-bit Quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Enable 4-bit quantization\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Use NF4 for better accuracy\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Compute in bfloat16 for better performance\n",
    "    bnb_4bit_use_double_quant=True  # Further reduce memory usage\n",
    ")\n",
    "\n",
    "# Load Model with 4-bit Quantization and Auto Device Mapping\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,  # Apply 4-bit quantization\n",
    "    device_map=\"auto\",  # Automatically map model to GPU\n",
    "    low_cpu_mem_usage=True,  # Avoid CPU offloading\n",
    "    token=hf_token\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99358f0790814d9b9778899b83571098"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:12:11.672672Z",
     "start_time": "2025-02-01T18:11:33.079572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Function to generate text\n",
    "def generate_text(prompt, max_length=512):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Who are you?\"\n",
    "output_text = generate_text(prompt)\n",
    "print(output_text)"
   ],
   "id": "1aa65ee17705f89d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who are you? How do you know me? And how do you know what I'm thinking? I thought I was alone in the world.\n",
      "I am an old man, with a long white beard and piercing eyes that see right through you. I have been watching you from afar, studying your thoughts and actions. I know your deepest desires and your darkest fears. And I know that you are not alone in this world, for you have a special gift â€“ a gift that sets you apart from all others.\n",
      "You have the power to communicate with the creatures of the forest, to hear their thoughts and to understand their language. It is a rare and precious gift, one that few others possess. And it is a gift that comes with great responsibility, for you must use it to help those who need your aid, and to protect the balance of nature in the world.\n",
      "But I sense that you are troubled, that you are unsure of how to use this gift, or even if you should use it at all. You are afraid of being different, of being seen as strange or unusual. And you are afraid of the power that this gift brings, and the weight of responsibility that comes with it.\n",
      "Do not be afraid, my young friend. Your gift is a blessing, and it is a part of who you are. Do not try to hide it or deny it, for that will only lead to suffering and pain. Instead, embrace it, and let it guide you on your journey through life. Use it to help others, and to make the world a better place. And always remember, you are not alone. There are others like you, who possess similar gifts and abilities. And there are those who would seek to use your gift for their own gain, and who would seek to harm you because of it.\n",
      "But do not worry, for you are strong and capable. You have the power to overcome any obstacle, and to achieve great things. And I will be here, watching over you, and guiding you whenever you need it. For I am your friend, and your mentor, and I will always be here to help you on your journey.\n",
      "Now, come, let us walk in the forest, and let us talk more of your gift, and of the adventures that await you. The trees are whispering secrets in the wind, and the creatures are waiting to meet you. Let us go, and let the journey begin.\n",
      "As we walk, the trees seem to grow taller and the path grows narrower. The air is filled with\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-01T18:08:10.648446Z",
     "start_time": "2025-02-01T18:07:59.766765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Option 2: Using transformers Pipeline\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=hf_token,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "outputs = pipeline(messages, max_new_tokens=256)\n",
    "print(outputs[0][\"generated_text\"][-1])\n",
    "\n",
    "# Option 3: Using Quantized Model for Memory Efficiency\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config, token=hf_token\n",
    ")\n",
    "\n",
    "input_text = \"What are we having for dinner?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "output = quantized_model.generate(**input_ids, max_new_tokens=10)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7780d5b127534da88856591510c9cf06"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['use_auth_token'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 15\u001B[0m\n\u001B[1;32m      2\u001B[0m pipeline \u001B[38;5;241m=\u001B[39m transformers\u001B[38;5;241m.\u001B[39mpipeline(\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext-generation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      4\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel_name,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      7\u001B[0m     use_auth_token\u001B[38;5;241m=\u001B[39mhf_token,\n\u001B[1;32m      8\u001B[0m )\n\u001B[1;32m     10\u001B[0m messages \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     11\u001B[0m     {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msystem\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are a pirate chatbot who always responds in pirate speak!\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m     12\u001B[0m     {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWho are you?\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m     13\u001B[0m ]\n\u001B[0;32m---> 15\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m256\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(outputs[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenerated_text\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Option 3: Using Quantized Model for Memory Efficiency\u001B[39;00m\n",
      "File \u001B[0;32m~/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:278\u001B[0m, in \u001B[0;36mTextGenerationPipeline.__call__\u001B[0;34m(self, text_inputs, **kwargs)\u001B[0m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(first_item, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mdict\u001B[39m)):\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001B[39;00m\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(first_item, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m--> 278\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mChat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext_inputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    279\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    280\u001B[0m         chats \u001B[38;5;241m=\u001B[39m (Chat(chat) \u001B[38;5;28;01mfor\u001B[39;00m chat \u001B[38;5;129;01min\u001B[39;00m text_inputs)  \u001B[38;5;66;03m# ðŸˆ ðŸˆ ðŸˆ\u001B[39;00m\n",
      "File \u001B[0;32m~/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1362\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1354\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[1;32m   1355\u001B[0m         \u001B[38;5;28miter\u001B[39m(\n\u001B[1;32m   1356\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1359\u001B[0m         )\n\u001B[1;32m   1360\u001B[0m     )\n\u001B[1;32m   1361\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1362\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1369\u001B[0m, in \u001B[0;36mPipeline.run_single\u001B[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[1;32m   1367\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[1;32m   1368\u001B[0m     model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpreprocess_params)\n\u001B[0;32m-> 1369\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1370\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(model_outputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpostprocess_params)\n\u001B[1;32m   1371\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1269\u001B[0m, in \u001B[0;36mPipeline.forward\u001B[0;34m(self, model_inputs, **forward_params)\u001B[0m\n\u001B[1;32m   1267\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[1;32m   1268\u001B[0m         model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_inputs, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m-> 1269\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1270\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_outputs, device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m   1271\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:383\u001B[0m, in \u001B[0;36mTextGenerationPipeline._forward\u001B[0;34m(self, model_inputs, **generate_kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeneration_config\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m generate_kwargs:\n\u001B[1;32m    381\u001B[0m     generate_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeneration_config\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgeneration_config\n\u001B[0;32m--> 383\u001B[0m generated_sequence \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    384\u001B[0m out_b \u001B[38;5;241m=\u001B[39m generated_sequence\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    385\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2012\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   2009\u001B[0m assistant_tokenizer \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124massistant_tokenizer\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)  \u001B[38;5;66;03m# only used for assisted generation\u001B[39;00m\n\u001B[1;32m   2011\u001B[0m generation_config, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_generation_config(generation_config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m-> 2012\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_model_kwargs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2013\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n\u001B[1;32m   2015\u001B[0m \u001B[38;5;66;03m# 2. Set generation parameters if not already defined\u001B[39;00m\n",
      "File \u001B[0;32m~/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1388\u001B[0m, in \u001B[0;36mGenerationMixin._validate_model_kwargs\u001B[0;34m(self, model_kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m         unused_model_args\u001B[38;5;241m.\u001B[39mappend(key)\n\u001B[1;32m   1387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m unused_model_args:\n\u001B[0;32m-> 1388\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1389\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe following `model_kwargs` are not used by the model: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00munused_model_args\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (note: typos in the\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1390\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m generate arguments will also show up in this list)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1391\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: The following `model_kwargs` are not used by the model: ['use_auth_token'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
